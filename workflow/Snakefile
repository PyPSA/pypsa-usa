from snakemake.utils import min_version
min_version("6.0")

from shutil import copyfile, move, rmtree
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

HTTP = HTTPRemoteProvider()

# -------------------------- Imports and Parameters -------------------------- #

from os.path import normpath
from pathlib import Path

FIGURES_SINGLE = [
    "capacity_map_base",
    "capacity_map_optimized",
    "capacity_map_new",
    "costs_bar",
    "production_bar",
    "production_area",
    "emissions_area",
    "emissions_accumulated",
    "emissions_accumulated_tech",
    "emissions_map",
    "renewable_potential_map",
    "capacity_additions_bar"
]

FIGURES_SINGLE_HTML = [
    "production_area_html",
    "emissions_area_html",
    # "emissions_node_html",
    "emissions_region_html",
    "emissions_accumulated_tech_html"
]

# --------------------------- Workflow constraints --------------------------- #

localrules:
    dag,
    clean,

wildcard_constraints:
    interconnect="usa|texas|western|eastern",
    simpl="[a-zA-Z0-9]*|all",
    clusters="[0-9]+m?|all",
    ll="(v|c)([0-9\.]+|opt|all)|all",
    opts="[-+a-zA-Z0-9\.]*",


# -------------------------- Config and Subworkflows ------------------------- #

# Merge subworkflow configs and main config
configfile: "config/config.default.yaml"
configfile: "config/config.cluster.yaml"
configfile: "config/config.osw.yaml"
configfile: "config/config.plotting.yaml"

ATLITE_NPROCESSES = config["atlite"].get("nprocesses", 4)

run = config.get("run", {})
RDIR = run["name"] + "/" if run.get("name") else ""
CDIR = RDIR if not run.get("shared_cutouts") else ""

LOGS = "logs/" + RDIR
BENCHMARKS = "benchmarks/" + RDIR
DATA = "data/" + RDIR
RESOURCES = "resources/" + RDIR if not run.get("shared_resources") else "resources/"
RESULTS = "results/" + RDIR

include: "rules/build_sector.smk"
include: "rules/postprocess.smk"

# ----------------------------------- Rules ---------------------------------- #

rule all:
    input:
        expand(
            "results/{interconnect}/figures/elec_s_{clusters}_ec_l{ll}_{opts}_{figure}.pdf",
            **config["scenarios"]["all"],
        ) if config['solving']['operations']['run'] 
        else 
            expand(
                "results/{interconnect}/figures/cluster_{clusters}/l{ll}_{opts}_{figure}.pdf",
                **config["scenarios"]["all"],
                figure=FIGURES_SINGLE
            ),
            # "repo_data/dag.jpg",

################# ----------- Rules to Retrieve Data ---------- #################

breakthrough_datafiles = [
    "bus.csv",
    "sub.csv",
    "bus2sub.csv",
    "branch.csv",
    "dcline.csv",
    "demand.csv",
    "plant.csv",
    "solar.csv",
    "wind.csv",
    "hydro.csv",
    "zone.csv",
]

pypsa_usa_datafiles = [
"gebco/gebco_2023_tid_USA.nc",
"copernicus/PROBAV_LC100_global_v3.0.1_2019-nrt_Discrete-Classification-map_USA_EPSG-4326.tif",
]

def define_zenodo_databundles():
    return {
        'USATestSystem':"https://zenodo.org/record/4538590/files/USATestSystem.zip",
        'pypsa_usa_data':"https://zenodo.org/records/10067222/files/pypsa_usa_data.zip?download=1"
        }

def define_sector_databundles():
    return {
        'pypsa_usa_sec':"https://zenodo.org/records/10067222/files/pypsa_usa_sec.zip?download=1"
        }

rule retrieve_zenodo_databundles:
    params:
        define_zenodo_databundles()
    output:
        expand(DATA + "breakthrough_network/base_grid/{file}", file=breakthrough_datafiles),
        expand(DATA + "{file}", file=pypsa_usa_datafiles),
        "data/natura.tiff",
    log:
        "logs/retrieve/retrieve_databundles.log",
    script:
        "scripts/retrieve_databundles.py"

sector_datafiles = [
    "counties/cb_2020_us_county_500k.shp",
    "population/DECENNIALDHC2020.P1-Data.csv",
    "urbanization/DECENNIALDHC2020.H1-Data.csv"
]

rule retrieve_sector_databundle:
    params:
        define_sector_databundles()
    output:
        expand(DATA + "{file}", file=sector_datafiles)
    log:
        LOGS + "retrieve_sector_databundle.log",
    retries: 2
    conda:
        "../envs/environment.yaml"
    script:
        "../scripts/retrieve_databundles.py"

rule retrieve_WECC_forecast_data:
    output:
        ads_2032 = directory(DATA + "WECC_ADS/downloads/2032/Public Data/Hourly Profiles in CSV format"),
        ads_2030 = directory(DATA + "WECC_ADS/downloads/2030/WECC 2030 ADS PCM 2020-12-16 (V1.5) Public Data/CSV Shape Files"),
        ads_dir = directory(DATA + "WECC_ADS/processed"),
    log:
        "logs/retrieve/retrieve_WECC_forecast_data.log",
    script:
        "scripts/retrieve_forecast_data.py"        

DATAFILES_DMD = [
    "EIA_DMD_2017.csv",
    "EIA_DMD_2018.csv",
    "EIA_DMD_2019.csv",
    "EIA_DMD_2020.csv",
    "EIA_DMD_2021.csv",
    "EIA_DMD_2022.csv",
    "EIA_DMD_2023.csv",
    ]

rule retrieve_eia_data:
    output:
        expand(RESOURCES + "eia/{file}", file=DATAFILES_DMD),
    log:
        "logs/retrieve/retrieve_historical_load_data.log",
    script:
        "scripts/retrieve_eia_data.py"


rule retrieve_ship_raster:
    input:
        HTTP.remote(
            "https://zenodo.org/record/6953563/files/shipdensity_global.zip",
            keep_local=True,
            static=True,
        ),
    output:
        DATA +"shipdensity_global.zip",
    log:
        LOGS + "retrieve_ship_raster.log",
    resources:
        mem_mb=5000,
    retries: 2
    run:
        move(input[0], output[0])


rule retrieve_cutout:
    input:
        HTTP.remote(
            'https://zenodo.org/records/10067222/files/{interconnect}_{cutout}.nc?download=1'
            ,static=True),
    output:
        "cutouts/" + CDIR + "{interconnect}_{cutout}.nc",
    log:
        "logs/" + CDIR + "retrieve_cutout_{interconnect}_{cutout}.log",
    resources:
        mem_mb=5000,
    retries: 2
    run:
        move(input[0], output[0])

rule retrieve_cost_data_eur:
    output:
        pypsa_technology_data = RESOURCES + "costs/{year}/pypsa_eur.csv",
    params:
        pypsa_costs_version = config["costs"].get("version", "v0.6.0")
    log:
        LOGS + "retrieve_cost_data_eur_{year}.log",
    resources:
        mem_mb=1000,
    script:
        "scripts/retrieve_cost_data_eur.py"

rule retrieve_cost_data_usa:
    output:
        nrel_atb = RESOURCES + "costs/nrel_atb.parquet",
        # nrel_atb_transport = RESOURCES + "costs/nrel_atb_transport.xlsx",
        ng_electric_power_price = RESOURCES + "costs/ng_electric_power_price.csv",
        ng_industrial_price = RESOURCES + "costs/ng_industrial_price.csv",
        ng_residential_price = RESOURCES + "costs/ng_commercial_price.csv",
        ng_commercial_price = RESOURCES + "costs/ng_residential_price.csv",
    params:
        eia_api_key = config["costs"].get("eia_aip_key", None),
    log:
        LOGS + "retrieve_cost_data_usa.log",
    resources:
        mem_mb=1000,
    script:
        "scripts/retrieve_cost_data_usa.py"

################# ----------- Rules to Build Network ---------- #################

rule build_shapes:
    params:
        source_states_shapes="admin_1_states_provinces",
        source_offshore_shapes=config["offshore_shape"],
        buffer_distance=200000,
    input:
        zone= DATA + "breakthrough_network/base_grid/zone.csv",
        onshore_shapes = "repo_data/BA_shapes_new/Modified_BE_BA_Shapes.shp",
        offshore_shapes = "repo_data/BOEM_CA_OSW_GIS/CA_OSW_BOEM_CallAreas.shp"
    output:
        country_shapes = RESOURCES + "{interconnect}/country_shapes.geojson",
        onshore_shapes = RESOURCES + "{interconnect}/onshore_shapes.geojson",
        offshore_shapes = RESOURCES + "{interconnect}/offshore_shapes.geojson",
        state_shapes = RESOURCES + "{interconnect}/state_boundaries.geojson"
    log:
        "logs/build_shapes_{interconnect}.log",
    threads: 1
    resources:
        mem_mb=1000,
    script:
        "scripts/build_shapes.py"

rule build_base_network:
    input:
        buses=DATA + "breakthrough_network/base_grid/bus.csv",
        lines=DATA + "breakthrough_network/base_grid/branch.csv",
        links=DATA + "breakthrough_network/base_grid/dcline.csv",
        bus2sub=DATA + "breakthrough_network/base_grid/bus2sub.csv",
        sub=DATA + "breakthrough_network/base_grid/sub.csv",
        onshore_shapes=RESOURCES + "{interconnect}/onshore_shapes.geojson",
        offshore_shapes=RESOURCES + "{interconnect}/offshore_shapes.geojson",
        state_shapes = RESOURCES + "{interconnect}/state_boundaries.geojson"
    output:
        bus2sub=DATA + "breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
        sub=DATA + "breakthrough_network/base_grid/{interconnect}/sub.csv",
        network=RESOURCES + "{interconnect}/elec_base_network.nc",
    log:
        "logs/create_network/{interconnect}.log",
    threads: 4
    resources:
        mem=500,
    script:
        "scripts/build_base_network.py"

rule build_bus_regions:
    input:
        country_shapes= RESOURCES + "{interconnect}/country_shapes.geojson",
        ba_region_shapes=RESOURCES + "{interconnect}/onshore_shapes.geojson",
        offshore_shapes=RESOURCES + "{interconnect}/offshore_shapes.geojson",
        base_network=RESOURCES + "{interconnect}/elec_base_network.nc",
        bus2sub="data/breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
        sub="data/breakthrough_network/base_grid/{interconnect}/sub.csv",
    output:
        regions_onshore=RESOURCES + "{interconnect}/regions_onshore.geojson",
        regions_offshore=RESOURCES + "{interconnect}/regions_offshore.geojson",
        # network=RESOURCES + "{interconnect}/elec_base_network_m.nc",
    log:
        "logs/{interconnect}/build_bus_regions_s.log",
    threads: 1
    resources:
        mem_mb=1000,
    script:
        "scripts/build_bus_regions.py"


rule build_cost_data:
    input:
        nrel_atb = RESOURCES + "costs/nrel_atb.parquet",
        pypsa_technology_data = RESOURCES + "costs/{year}/pypsa_eur.csv",
    output:
        tech_costs = DATA + "costs_{year}.csv",
    log:
        LOGS + "costs_{year}.log",
    script:
        "scripts/build_cost_data.py"


if config["enable"].get("build_cutout", False):
    rule build_cutout:
        params:
            snapshots=config["snapshots"],
            cutouts=config["atlite"]["cutouts"],
            interconnects=config["atlite"]["interconnects"],
        input:
            regions_onshore = RESOURCES + "{interconnect}/country_shapes.geojson",
            regions_offshore = RESOURCES + "{interconnect}/offshore_shapes.geojson",
        output:
            protected("cutouts/" + CDIR + "{interconnect}_{cutout}.nc"),
        log:
            "logs/" + CDIR + "build_cutout/{interconnect}_{cutout}.log",
        benchmark:
            "benchmarks/" + CDIR + "build_cutout_{interconnect}_{cutout}"
        threads: ATLITE_NPROCESSES
        resources:
            mem_mb=ATLITE_NPROCESSES * 1000,
        conda:
            "../envs/environment.yaml"
        script:
            "../scripts/build_cutout.py"


rule build_ship_raster:
    input:
        ship_density=DATA + "shipdensity_global.zip",
        cutouts=expand(
            "cutouts/" + CDIR + "western_{cutout}.nc",
            cutout=[
                config["renewable"][carrier]["cutout"]
                for carrier in config["electricity"]["renewable_carriers"]
            ],
        ),
    output:
        RESOURCES + "{interconnect}/shipdensity_raster.tif",
    log:
        LOGS + "{interconnect}/build_ship_raster.log",
    resources:
        mem_mb=5000,
    benchmark:
        BENCHMARKS + "{interconnect}/build_ship_raster"
    script:
        "subworkflows/pypsa-eur/scripts/build_ship_raster.py"

rule build_hydro_profiles:
    params:
        hydro=config["renewable"]["hydro"],
        countries=config["countries"],
    input:
        ba_region_shapes=RESOURCES + "{interconnect}/onshore_shapes.geojson",
        # eia_hydro_generation="data/eia_hydro_annual_generation.csv",
        cutout=f"cutouts/" + CDIR + "{interconnect}_" + config["renewable"]["hydro"]["cutout"] + ".nc",
    output:
        RESOURCES + "{interconnect}/profile_hydro.nc",
    log:
        LOGS + "{interconnect}/build_hydro_profile.log",
    resources:
        mem_mb=5000,
    conda:
        "envs/environment.yaml"
    script:
        "scripts/build_hydro_profile.py"

rule build_renewable_profiles:
    params:
        renewable=config["renewable"],
    input:
        base_network= RESOURCES + "{interconnect}/elec_base_network.nc",
        corine=ancient("data/copernicus/PROBAV_LC100_global_v3.0.1_2019-nrt_Discrete-Classification-map_USA_EPSG-4326.tif"),
        natura=lambda w: (
            DATA + "natura.tiff"
            if config["renewable"][w.technology]["natura"]
            else []
        ),
        gebco=ancient(
            lambda w: (
                DATA + "gebco/gebco_2023_tid_USA.nc"
                if config["renewable"][w.technology].get("max_depth")
                else []
            )
        ),
        ship_density=lambda w: (
            RESOURCES + "{interconnect}/shipdensity_raster.tif"
            if "ship_threshold" in config["renewable"][w.technology].keys()
            else []
        ),
        country_shapes=RESOURCES + "{interconnect}/country_shapes.geojson", 
        offshore_shapes=RESOURCES + "{interconnect}/offshore_shapes.geojson",
        regions=lambda w: (
            RESOURCES + "{interconnect}/regions_onshore.geojson"
            if w.technology in ("onwind", "solar")
            else RESOURCES + "{interconnect}/regions_offshore.geojson"
        ),
        cutout=lambda w: "cutouts/"
        + CDIR + "{interconnect}_"
        + config["renewable"][w.technology]["cutout"]
        + ".nc",
    output:
        profile=RESOURCES + "{interconnect}/profile_{technology}.nc",
    log:
        LOGS + "{interconnect}/build_renewable_profile_{technology}.log",
    benchmark:
        BENCHMARKS + "{interconnect}/build_renewable_profiles_{technology}"
    threads: ATLITE_NPROCESSES
    resources:
        mem_mb=ATLITE_NPROCESSES * 5000,
    wildcard_constraints:
        technology="(?!hydro).*",  # Any technology other than hydro
    script:
        "scripts/build_renewable_profiles.py"

rule add_electricity:
    params:
        length_factor=config["lines"]["length_factor"],
        scaling_factor=config["load"]["scaling_factor"],
        countries=config["countries"],
        renewable=config["renewable"],
        electricity=config["electricity"],
        conventional=config["conventional"],
        costs=config["costs"],
    input:
        **{
            f"profile_{tech}": RESOURCES + "{interconnect}" + f"/profile_{tech}.nc"
            for tech in config["electricity"]["renewable_carriers"]
            if tech != "hydro"
        },
        **{
            f"conventional_{carrier}_{attr}": fn
            for carrier, d in config.get("conventional", {None: {}}).items()
            if carrier in config["electricity"]["conventional_carriers"]
            for attr, fn in d.items()
            if str(fn).startswith("data/")
        },
        base_network=RESOURCES + "{interconnect}/elec_base_network.nc",
        tech_costs=DATA + f"costs_{config['costs']['year']}.csv",
        regions=RESOURCES + "{interconnect}/regions_onshore.geojson",
        plants_eia="repo_data/eia_plants_wecc.csv",
        plants_ads="repo_data/ads_plants_locs.csv",
        fuel_costs="repo_data/eia_mappings/fuelCost22.csv",
        plants_breakthrough="data/breakthrough_network/base_grid/plant.csv",
        hydro_breakthrough="data/breakthrough_network/base_grid/hydro.csv",
        wind_breakthrough="data/breakthrough_network/base_grid/wind.csv",
        solar_breakthrough="data/breakthrough_network/base_grid/solar.csv",
        ads_renewables = "data/WECC_ADS/processed/",
        bus2sub="data/breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
        demand_breakthrough_2016=DATA + "breakthrough_network/base_grid/demand.csv",
        ads_2032 = DATA + "WECC_ADS/downloads/2032/Public Data/Hourly Profiles in CSV format",
        ads_2030 = DATA + "WECC_ADS/downloads/2030/WECC 2030 ADS PCM 2020-12-16 (V1.5) Public Data/CSV Shape Files",
        eia = expand(RESOURCES + "eia/{file}", file=DATAFILES_DMD),
        **{
            f"gen_cost_mult_{Path(x).stem}":f"repo_data/locational_multipliers/{Path(x).name}" for x in Path("repo_data/locational_multipliers/").glob("*")
        },
        ng_electric_power_price = RESOURCES + "costs/ng_electric_power_price.csv",
    output:
        RESOURCES + "{interconnect}/elec_base_network_l_pp.nc",
    log:
        LOGS + "{interconnect}_add_electricity.log",
    benchmark:
        BENCHMARKS + "{interconnect}_add_electricity"
    threads: 1
    resources:
        mem_mb=5000,
    script:
        "scripts/add_electricity.py"

################# ----------- Rules to Aggregate & Simplify Network ---------- #################
rule simplify_network:
    input:
        bus2sub="data/breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
        sub="data/breakthrough_network/base_grid/{interconnect}/sub.csv",
        network= RESOURCES + "{interconnect}/elec_base_network_l_pp.nc",
    output:
        network=RESOURCES + "{interconnect}/elec_s.nc",
    log:
        "logs/simplify_network/{interconnect}/elec_s.log",
    threads: 4
    resources:
        mem=500,
    script:
        "scripts/simplify_network.py"


rule cluster_network:
    input:
        network=RESOURCES + "{interconnect}/elec_s.nc",
        regions_onshore=RESOURCES + "{interconnect}/regions_onshore.geojson",
        regions_offshore=RESOURCES + "{interconnect}/regions_offshore.geojson",
        busmap="data/breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
        custom_busmap=(
            "data/{interconnect}/custom_busmap_{clusters}.csv"
            if config["enable"].get("custom_busmap", False)
            else []
        ),
        tech_costs=DATA + f"costs_{config['costs']['year']}.csv",
    output:
        network=RESOURCES + "{interconnect}/elec_s_{clusters}.nc",
        regions_onshore=RESOURCES + "{interconnect}/regions_onshore_s_{clusters}.geojson",
        regions_offshore=RESOURCES + "{interconnect}/regions_offshore_s_{clusters}.geojson",
        busmap=RESOURCES + "{interconnect}/busmap_s_{clusters}.csv",
        linemap=RESOURCES + "{interconnect}/linemap_s_{clusters}.csv",
    log:
        "logs/cluster_network/{interconnect}/elec_s_{clusters}.log",
    benchmark:
        "benchmarks/cluster_network/{interconnect}/elec_s_{clusters}"
    threads: 1
    resources:
        mem_mb=6000,
    script:
        "scripts/cluster_network_eur.py"

################# ----------- Rules to Optimize/Solve Network ---------- #################

rule add_extra_components:
    input:
        network=RESOURCES + "{interconnect}/elec_s_{clusters}.nc",
        tech_costs=DATA + f"costs_{config['costs']['year']}.csv",
    output:
        RESOURCES + "{interconnect}/elec_s_{clusters}_ec.nc",
    log:
        "logs/add_extra_components/{interconnect}/elec_s_{clusters}_ec.log",
    threads: 4
    resources:
        mem=500,
    script:
        "scripts/add_extra_components.py"

rule prepare_network:
    params:
        links=config["links"],
        lines=config["lines"],
        co2base=config["electricity"]["co2base"],
        co2limit=config["electricity"]["co2limit"],
        gaslimit=config["electricity"].get("gaslimit"),
        max_hours=config["electricity"]["max_hours"],
        costs=config["costs"],
    input:
        network=RESOURCES + "{interconnect}/elec_s_{clusters}_ec.nc",
        tech_costs=DATA + f"costs_{config['costs']['year']}.csv",
    output:
        RESOURCES + "{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
    log:
        solver="logs/prepare_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}.log",
    threads: 4
    resources:
        mem=5000,
    log:
        "logs/prepare_network",
    script:
        "scripts/subworkflows/pypsa-eur/scripts/prepare_network.py" 

def memory(w):
    factor = 3.0
    for o in w.opts.split("-"):
        m = re.match(r"^(\d+)h$", o, re.IGNORECASE)
        if m is not None:
            factor /= int(m.group(1))
            break
    for o in w.opts.split("-"):
        m = re.match(r"^(\d+)seg$", o, re.IGNORECASE)
        if m is not None:
            factor *= int(m.group(1)) / 8760
            break
    if w.clusters.endswith("m"):
        return int(factor * (18000 + 180 * int(w.clusters[:-1])))
    elif w.clusters == "all":
        return int(factor * (18000 + 180 * 4000))
    else:
        return int(factor * (10000 + 195 * int(w.clusters)))


rule solve_network:
    input:
        RESOURCES + "{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
    output:
        "results/{interconnect}/networks/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
    log:
        solver=normpath(
            "logs/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}_solver.log"
        ),
        python="logs/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}_python.log",
        memory="logs/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}_memory.log",
    benchmark:
        "benchmarks/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}"
    threads: 8
    resources:
        mem_mb=memory,
    script:
        "scripts/solve_network.py"


# Create DAG with- 
# snakemake --dag -F | sed -n "/digraph/,\$p" | dot -Tpng -o repo_data/dag.jpg
# snakemake --rulegraph all | sed -n "/digraph/,\$p" | dot -Tpng -o repo_data/dag.jpg
rule dag:
    message:
        "Creating DAG of workflow."
    output:
        dot="repo_data/dag.dot",
        jpg="repo_data/dag.jpg",
    shell:
        """
        snakemake --rulegraph all | sed -n "/digraph/,\$p" > {output.dot}
        dot -Tjpg -o {output.jpg} {output.dot}
        """

rule clean:
    message:
        "Remove all build results but keep downloaded data."
    run:
        import shutil

        shutil.rmtree("resources", ignore_errors=True)
        shutil.rmtree("results", ignore_errors=True)
        print("Data downloaded to data/ has not been cleaned.")
